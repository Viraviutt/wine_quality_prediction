# üçæ Predicci√≥n y Generaci√≥n de Insights sobre Calidad de Vinos con MLOps

Este proyecto implementa un flujo de MLOps de principio a fin para predecir la calidad de vinos (escala 0-10) utilizando un modelo de Machine Learning (Random Forest) y sirve las predicciones a trav√©s de una interfaz web con Gradio. El proyecto utiliza **MLflow** para el seguimiento de experimentos, el registro de modelos y el control del ciclo de vida.

## ‚ú® Caracter√≠sticas Principales de la Aplicaci√≥n

| Caracter√≠stica | Herramienta | Descripci√≥n |
| :--- | :--- | :--- |
| **Seguimiento MLOps** | MLflow | **Registra cada predicci√≥n** realizada por la interfaz Gradio como un nuevo *Run*, guardando los *inputs* y *outputs* para trazabilidad. |
| **Model Registry** | MLflow | Carga la versi√≥n del modelo de mayor calidad (etiquetada como `status: production`) directamente desde el Registro de Modelos. |
| **Predicci√≥n Individual** | Gradio | Interfaz con 11 campos num√©ricos para ingresar manualmente las propiedades fisicoqu√≠micas del vino. |
| **Predicci√≥n por Lote** | Gradio | Permite subir un archivo CSV para obtener predicciones en masa, registrando el archivo y los resultados en MLflow. |
| **Explicaciones Gen AI** | Gemini API | Utiliza un Large Language Model (LLM) para generar una explicaci√≥n de texto (rol de "Sommelier Virtual") que justifica la predicci√≥n del modelo de ML. |

## ‚öôÔ∏è Pre-requisitos

Para instalar y ejecutar este proyecto, necesitas tener instalado:

* **Python 3.11.14**
* **Conda/Mamba** (Recomendado para la gesti√≥n de entornos)
* Una clave de la **Gemini API**.

## üöÄ Gu√≠a de Instalaci√≥n y Ejecuci√≥n

Sigue estos pasos detallados para configurar y lanzar la aplicaci√≥n en una nueva m√°quina.

### Paso 1: Clonar el Repositorio y Configurar el Entorno

```bash
# 1. Clonar tu repositorio (reemplaza con tu URL real si aplica)
git clone https://github.com/Viraviutt/wine_quality_prediction
cd wine_quality_prediction

# 2. Crear y activar el entorno virtual con Conda
conda env create -f conda.yaml
conda activate wine-mlops-env

# 3. Instalar las dependencias
pip install -r requirements.txt 
# Si no tienes requirements.txt, instala las librer√≠as principales:
# pip install mlflow scikit-learn pandas numpy gradio python-dotenv openai google-generativeai
```
### Paso 2: Configurar las Variables de Entorno y el Backend

La aplicaci√≥n requiere la clave de Gemini para las explicaciones y una base de datos SQLite para MLflow.

```bash
# 1. Configurar la API KEY
GEMINI_API_KEY=<tu_api_key>

# 2. Configurar la Base de Datos de MLflow
touch mlflow.db
conda activate wine-mlops-env
```

### 3. Iniciar el Servidor de MLflow
El servidor de MLflow debe estar activo para que la aplicaci√≥n Gradio pueda cargar el modelo y hacer el logging de las predicciones a trav√©s de la API HTTP.

```bash
mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./mlruns \
    --host 0.0.0.0 \
    --port 5000
```

### 4. Entrenamiento y Registro del Modelo
Debes ejecutar el script de entrenamiento para generar el modelo y registrarlo en la versi√≥n status: stagging dentro de la base de datos mlflow.db.

```bash
export MLFLOW_TRACKING_URI=sqlite:///mlflow.db
mlflow run project
```
*NOTA*: Deja esta terminal ejecutando el servidor y abre una nueva terminal para el siguiente paso.

### Paso 5: Ejecutar la Aplicaci√≥n Gradio
En la nueva terminal (con el entorno wine-mlops-env activado), ejecuta la aplicaci√≥n web. El script app.py se conectar√° a http://localhost:5000 para cargar el modelo en producci√≥n.

```bash
# Aseg√∫rate de estar en el directorio correcto y el entorno activado
python app.py
```

Tu navegador se abrir√° autom√°ticamente en la direcci√≥n http://127.0.0.1:7860. Cada interacci√≥n en Gradio (predicci√≥n individual o lote) ahora se registrar√° como un nuevo Run en tu servidor de MLflow.